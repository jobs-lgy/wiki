(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{205:function(t,a,s){"use strict";s.r(a);var e=s(0),n=Object(e.a)({},function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"mapreduce"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce","aria-hidden":"true"}},[t._v("#")]),t._v(" MapReduce")]),t._v(" "),s("h2",{attrs:{id:"mapreduce概述"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce概述","aria-hidden":"true"}},[t._v("#")]),t._v(" MapReduce概述")]),t._v(" "),s("p",[t._v("Mapreduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架；")]),t._v(" "),s("p",[t._v("Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。")]),t._v(" "),s("h2",{attrs:{id:"为什么要mapreduce"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#为什么要mapreduce","aria-hidden":"true"}},[t._v("#")]),t._v(" 为什么要MapReduce")]),t._v(" "),s("p",[t._v("1）海量数据在单机上处理因为硬件资源限制，无法胜任")]),t._v(" "),s("p",[t._v("2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度")]),t._v(" "),s("p",[t._v("3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理。")]),t._v(" "),s("p",[t._v("4）mapreduce分布式方案考虑的问题")]),t._v(" "),s("p",[t._v("（1）运算逻辑要不要先分后合？")]),t._v(" "),s("p",[t._v("（2）程序如何分配运算任务（切片）？")]),t._v(" "),s("p",[t._v("（3）两阶段的程序如何启动？如何协调？")]),t._v(" "),s("p",[t._v("（4）整个程序运行过程中的监控？容错？重试？")]),t._v(" "),s("p",[t._v("分布式方案需要考虑很多问题，但是我们可以将分布式程序中的公共功能封装成框架，让开发人员将精力集中于业务逻辑上，而mapreduce就是这样一个分布式程序的通用框架。")]),t._v(" "),s("h3",{attrs:{id:"核心思想"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#核心思想","aria-hidden":"true"}},[t._v("#")]),t._v(" 核心思想")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww1.sinaimg.cn/large/006tNc79gy1g5acqyjtcej31ip0pshdm.jpg",alt:"img"}})]),t._v(" "),s("p",[t._v("1）分布式的运算程序往往需要分成至少2个阶段")]),t._v(" "),s("p",[t._v("2）第一个阶段的maptask并发实例，完全并行运行，互不相干")]),t._v(" "),s("p",[t._v("3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出")]),t._v(" "),s("p",[t._v("4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行")]),t._v(" "),s("p",[s("strong",[t._v("MapReduce")]),t._v("进程")]),t._v(" "),s("p",[t._v("一个完整的mapreduce程序在分布式运行时有三类实例进程：")]),t._v(" "),s("p",[t._v("1）MrAppMaster：负责整个程序的过程调度及状态协调")]),t._v(" "),s("p",[t._v("2）MapTask：负责map阶段的整个数据处理流程")]),t._v(" "),s("p",[t._v("3）ReduceTask：负责reduce阶段的整个数据处理流程")]),t._v(" "),s("h3",{attrs:{id:"编程规范"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#编程规范","aria-hidden":"true"}},[t._v("#")]),t._v(" 编程规范")]),t._v(" "),s("p",[t._v("用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行mr程序的客户端)")]),t._v(" "),s("p",[t._v("1）Mapper阶段")]),t._v(" "),s("p",[t._v("​\t（1）用户自定义的Mapper要继承自己的父类")]),t._v(" "),s("p",[t._v("​\t（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）")]),t._v(" "),s("p",[t._v("​\t（3）Mapper中的业务逻辑写在map()方法中")]),t._v(" "),s("p",[t._v("​\t（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）")]),t._v(" "),s("p",[t._v("​\t（5）map()方法（maptask进程）对每一个<K,V>调用一次")]),t._v(" "),s("p",[t._v("2）Reducer阶段")]),t._v(" "),s("p",[t._v("​\t（1）用户自定义的Reducer要继承自己的父类")]),t._v(" "),s("p",[t._v("​\t（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV")]),t._v(" "),s("p",[t._v("​\t（3）Reducer的业务逻辑写在reduce()方法中")]),t._v(" "),s("p",[t._v("​\t（4）Reducetask进程对每一组相同k的<k,v>组调用一次reduce()方法")]),t._v(" "),s("p",[t._v("3）Driver阶段")]),t._v(" "),s("p",[t._v("整个程序需要一个Drvier来进行提交，提交的是一个描述了各种必要信息的job对象")]),t._v(" "),s("h3",{attrs:{id:"mapreduce运行流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapreduce运行流程","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("MapReduce")]),t._v("运行流程")]),t._v(" "),s("h4",{attrs:{id:"运行机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#运行机制","aria-hidden":"true"}},[t._v("#")]),t._v(" 运行机制")]),t._v(" "),s("p",[t._v("1）在MapReduce程序读取文件的输入目录上存放相应的文件。")]),t._v(" "),s("p",[t._v("2）客户端程序在submit()方法执行前，获取待处理的数据信息，然后根据集群中参数的配置形成一个任务分配规划。")]),t._v(" "),s("p",[t._v("3）客户端提交job.split、jar包、job.xml等文件给yarn，yarn中的resourcemanager启动MRAppMaster。")]),t._v(" "),s("p",[t._v("4）MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程。")]),t._v(" "),s("p",[t._v("5）maptask利用客户指定的inputformat来读取数据，形成输入KV对。")]),t._v(" "),s("p",[t._v("6）maptask将输入KV对传递给客户定义的map()方法，做逻辑运算")]),t._v(" "),s("p",[t._v("7）map()运算完毕后将KV对收集到maptask缓存。")]),t._v(" "),s("p",[t._v("8）maptask缓存中的KV对按照K分区排序后不断写到磁盘文件")]),t._v(" "),s("p",[t._v("9）MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据分区。")]),t._v(" "),s("p",[t._v("10）Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算。")]),t._v(" "),s("p",[t._v("11）Reducetask运算完毕后，调用客户指定的outputformat将结果数据输出到外部存储。")]),t._v(" "),s("p",[t._v("工作机制：")]),t._v(" "),s("p",[t._v("更详细的过程：")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww2.sinaimg.cn/large/006tNc79gy1g5acr96dfrj31kw0rttjy.jpg",alt:"img"}})]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww3.sinaimg.cn/large/006tNc79gy1g5acreaxdvj31l60r0114.jpg",alt:"img"}})]),t._v(" "),s("p",[t._v("上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下：")]),t._v(" "),s("p",[t._v("1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中")]),t._v(" "),s("p",[t._v("2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件")]),t._v(" "),s("p",[t._v("3）多个溢出文件会被合并成大的溢出文件")]),t._v(" "),s("p",[t._v("4）在溢出及合并的过程中，都要调用partitioner进行分区和针对key进行排序")]),t._v(" "),s("p",[t._v("5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据")]),t._v(" "),s("p",[t._v("6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）")]),t._v(" "),s("p",[t._v("7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）")]),t._v(" "),s("blockquote",[s("p",[t._v("注意：Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。")]),t._v(" "),s("p",[t._v("缓冲区的大小可以通过参数调整，参数：io.sort.mb  默认100M")])]),t._v(" "),s("h2",{attrs:{id:"inputformat切片机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#inputformat切片机制","aria-hidden":"true"}},[t._v("#")]),t._v(" InputFormat切片机制")]),t._v(" "),s("h3",{attrs:{id:"fileinputformat"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fileinputformat","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("FileInputFormat")])]),t._v(" "),s("p",[s("strong",[t._v("1）job提交流程源码详解")])]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token function"}},[t._v("waitForCompletion")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("submit")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 1建立连接")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("connect")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\t\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 1）创建提交job的代理")]),t._v("\n \t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Cluster")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getConfiguration")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// （1）判断是本地yarn还是远程")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("initialize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("jobTrackAddr"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2 提交job")]),t._v("\n\tsubmitter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("submitJobInternal")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Job")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("this")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cluster"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 1）创建给集群提交数据的Stag路径")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Path")]),t._v(" jobStagingArea "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobSubmissionFiles")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("*getStagingDir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cluster"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2）获取jobid ，并创建job路径")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobID")]),t._v(" jobId "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" submitClient"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getNewJobID")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 3）拷贝jar包到集群")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("copyAndConfigureFiles")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" submitJobDir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\t\n\trUploader"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("uploadFiles")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" jobSubmitDir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 4）计算切片，生成切片规划文件")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeSplits")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" submitJobDir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tmaps "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeNewSplits")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" jobSubmitDir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tinput"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getSplits")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 5）向Stag路径写xml配置文件")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeConf")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("conf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" submitJobFile"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\tconf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeXml")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("out"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 6）提交job,返回提交状态")]),t._v("\n\tstatus "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" submitClient"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("submitJob")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("jobId"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" submitJobDir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("toString")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getCredentials")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"http://ww1.sinaimg.cn/large/006tNc79gy1g5acrjky9cj31p60u047e.jpg",alt:"img"}})]),t._v(" "),s("p",[s("strong",[t._v("2）FileInputFormat源码解析(input.getSplits(job))")])]),t._v(" "),s("p",[t._v("（1）找到你数据存储的目录。")]),t._v(" "),s("p",[t._v("（2）开始遍历处理（规划切片）目录下的每一个文件")]),t._v(" "),s("p",[t._v("（3）遍历第一个文件ss.txt")]),t._v(" "),s("p",[t._v("​\t\ta）获取文件大小fs.sizeOf(ss.txt);")]),t._v(" "),s("p",[t._v("​\t\tb）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M")]),t._v(" "),s("p",[t._v("​\t\tc）默认情况下，切片大小=blocksize")]),t._v(" "),s("p",[t._v("​\t\td）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）")]),t._v(" "),s("p",[t._v("​\t\te）将切片信息写到一个切片规划文件中")]),t._v(" "),s("p",[t._v("​\t\tf）整个切片的核心过程在getSplit()方法中完成。")]),t._v(" "),s("p",[t._v("​\t\tg）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。")]),t._v(" "),s("p",[t._v("​\t\th）注意：block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分")]),t._v(" "),s("p",[t._v("（4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。")]),t._v(" "),s("p",[s("strong",[t._v("3）FileInputFormat中默认的切片机制：")])]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww2.sinaimg.cn/large/006tNc79gy1g5acrp3l4bj31it0ofk51.jpg",alt:"img"}})]),t._v(" "),s("p",[t._v("（1）简单地按照文件的内容长度进行切片")]),t._v(" "),s("p",[t._v("（2）切片大小，默认等于block大小")]),t._v(" "),s("p",[t._v("（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片")]),t._v(" "),s("p",[t._v("比如待处理数据有两个文件：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v(" file1.txt    320M  \n file2.txt    10M \n")])])]),s("p",[t._v("经过FileInputFormat的切片机制运算后，形成的切片信息如下：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("file1.txt.split1--  0~128 \nfile1.txt.split2--  128~256 \nfile1.txt.split3--  256~320 \nfile2.txt.split1--  0~10M \n")])])]),s("p",[s("strong",[t._v("4）FileInputFormat切片大小的参数配置")])]),t._v(" "),s("p",[t._v("（1）通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize));")]),t._v(" "),s("p",[t._v("切片主要由这几个值来运算决定")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("mapreduce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fileinputformat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("minsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//默认值为1")]),t._v("\n\nmapreduce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fileinputformat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("maxsize"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Long")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MAXValue")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//默认值Long.MAXValue")]),t._v("\n")])])]),s("p",[t._v("因此，默认情况下，切片大小=blocksize。")]),t._v(" "),s("p",[t._v("maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。")]),t._v(" "),s("p",[t._v("minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。")]),t._v(" "),s("p",[s("strong",[t._v("5）获取切片信息API")])]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 根据文件类型获取切片信息")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSplit")]),t._v(" inputSplit "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FileSplit")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getInputSplit")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 获取切片的文件名称")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" inputSplit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getPath")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getName")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("h3",{attrs:{id:"combinetextinputformat"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#combinetextinputformat","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("CombineTextInputFormat")])]),t._v(" "),s("p",[t._v("关于大量小文件的优化策略")]),t._v(" "),s("p",[t._v("1）默认情况下TextInputformat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。")]),t._v(" "),s("p",[t._v("2）优化策略")]),t._v(" "),s("p",[t._v("​\t（1）最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到HDFS做后续分析。")]),t._v(" "),s("p",[t._v("​\t（2）补救措施：如果已经是大量小文件在HDFS中了，可以使用另一种InputFormat来做切片（CombineTextInputFormat），它的切片逻辑跟TextFileInputFormat不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个maptask。")]),t._v(" "),s("p",[t._v("​\t（3）优先满足最小切片大小，不超过最大切片大小")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CombineTextInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMaxInputSplitSize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4194304")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 128m")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CombineTextInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMinInputSplitSize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2097152")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2m")]),t._v("\n")])])]),s("p",[t._v("​\t举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m")]),t._v(" "),s("p",[t._v("3）具体实现步骤")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 如果不设置InputFormat,它默认用的是TextInputFormat.class")]),t._v("\njob"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setInputFormatClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CombineTextInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CombineTextInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMaxInputSplitSize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4194304")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 4m")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CombineTextInputFormat")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setMinInputSplitSize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2097152")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2m")]),t._v("\n")])])]),s("h2",{attrs:{id:"maptask机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#maptask机制","aria-hidden":"true"}},[t._v("#")]),t._v(" MapTask机制")]),t._v(" "),s("h3",{attrs:{id:"并行度决定机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#并行度决定机制","aria-hidden":"true"}},[t._v("#")]),t._v(" 并行度决定机制")]),t._v(" "),s("p",[t._v("1）问题引出")]),t._v(" "),s("p",[t._v("maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度。那么，mapTask并行任务是否越多越好呢？")]),t._v(" "),s("p",[t._v("2）MapTask并行度决定机制")]),t._v(" "),s("p",[t._v("​\t"),s("strong",[t._v("一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。")])]),t._v(" "),s("h3",{attrs:{id:"maptask工作机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#maptask工作机制","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("MapTask")]),t._v("工作机制")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww2.sinaimg.cn/large/006tNc79gy1g5acrw1v1sj31s40u0aki.jpg",alt:"img"}})]),t._v(" "),s("p",[t._v("​\t（1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。")]),t._v(" "),s("p",[t._v("​\t（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。")]),t._v(" "),s("p",[t._v("​\t（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。")]),t._v(" "),s("p",[t._v("​\t（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。")]),t._v(" "),s("p",[t._v("​\t"),s("strong",[t._v("溢写阶段详情：")])]),t._v(" "),s("p",[t._v("​\t步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。")]),t._v(" "),s("p",[t._v("​\t步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。")]),t._v(" "),s("p",[t._v("​\t步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。")]),t._v(" "),s("p",[t._v("​\t（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。")]),t._v(" "),s("p",[t._v("​\t当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。")]),t._v(" "),s("p",[t._v("​\t在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。")]),t._v(" "),s("p",[t._v("​\t让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。")]),t._v(" "),s("h2",{attrs:{id:"shuffle机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#shuffle机制","aria-hidden":"true"}},[t._v("#")]),t._v(" Shuffle机制")]),t._v(" "),s("h3",{attrs:{id:"shuffle机制-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#shuffle机制-2","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("Shuffle")]),t._v("机制")]),t._v(" "),s("p",[t._v("Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww4.sinaimg.cn/large/006tNc79gy1g5aacqkh6yj31o90u0n9j.jpg",alt:"img"}})]),t._v(" "),s("h3",{attrs:{id:"partition分区"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#partition分区","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("partition")]),t._v("分区")]),t._v(" "),s("p",[t._v("0）问题引出")]),t._v(" "),s("p",[t._v("要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）")]),t._v(" "),s("p",[t._v("1）默认partition分区")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("HashPartitioner")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Partitioner")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("/** Use {@link Object#hashCode()} to partition. */")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getPartition")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("K")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("V")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" numReduceTasks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("hashCode")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("&")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Integer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MAX_VALUE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" numReduceTasks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("​\t默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。")]),t._v(" "),s("p",[t._v("2）自定义Partitioner步骤")]),t._v(" "),s("p",[t._v("​\t（1）自定义类继承Partitioner，重写getPartition()方法")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("ProvincePartitioner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Partitioner")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FlowBean")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getPartition")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FlowBean")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" numPartitions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 1 获取电话号码的前三位")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" preNum "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("toString")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("substring")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\n\t\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" partition "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t\n\t\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 2 判断是哪个省")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"136"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("equals")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preNum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tpartition "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"137"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("equals")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preNum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tpartition "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"138"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("equals")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preNum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tpartition "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"139"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("equals")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preNum"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tpartition "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" partition"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("​\t（2）在job驱动中，设置自定义partitioner：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setPartitionerClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("CustomPartitioner")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("​\t（3）自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce task")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setNumReduceTasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n")])])]),s("p",[t._v("3）注意：")]),t._v(" "),s("p",[t._v("如果reduceTask的数量> getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；")]),t._v(" "),s("p",[t._v("如果1<reduceTask的数量<getPartition的结果数，则有一部分分区数据无处安放，会Exception；")]),t._v(" "),s("p",[t._v("如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000；")]),t._v(" "),s("p",[t._v("​\t例如：假设自定义分区数为5，则")]),t._v(" "),s("p",[t._v("（1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件")]),t._v(" "),s("p",[t._v("（2）job.setNumReduceTasks(2);会报错")]),t._v(" "),s("p",[t._v("（3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件")]),t._v(" "),s("h3",{attrs:{id:"writablecomparable排序"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#writablecomparable排序","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("WritableComparable")]),t._v("排序")]),t._v(" "),s("p",[t._v("排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据（按照key）进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。")]),t._v(" "),s("p",[t._v("对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。")]),t._v(" "),s("p",[t._v("对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。")]),t._v(" "),s("p",[t._v("每个阶段的默认排序：")]),t._v(" "),s("p",[s("strong",[t._v("1）排序的分类：")])]),t._v(" "),s("p",[t._v("​\t（1）部分排序：")]),t._v(" "),s("p",[t._v("MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。")]),t._v(" "),s("p",[t._v("​\t（2）全排序：")]),t._v(" "),s("p",[t._v("如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。")]),t._v(" "),s("p",[t._v("​\t替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。")]),t._v(" "),s("p",[t._v("（3）辅助排序：（GroupingComparator分组）")]),t._v(" "),s("p",[t._v("​\tMapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。")]),t._v(" "),s("p",[t._v("​\t（4）二次排序：")]),t._v(" "),s("p",[t._v("​\t在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。")]),t._v(" "),s("p",[s("strong",[t._v("2）自定义排序WritableComparable")])]),t._v(" "),s("p",[t._v("bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("compareTo")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("FlowBean")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 倒序排列，从大到小")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("this")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sumFlow "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getSumFlow")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h3",{attrs:{id:"groupingcomparator"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#groupingcomparator","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("GroupingComparator")])]),t._v(" "),s("p",[t._v("对reduce阶段的数据根据某一个或几个字段进行分组。")]),t._v(" "),s("h3",{attrs:{id:"combiner合并"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#combiner合并","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("Combiner")]),t._v("合并")]),t._v(" "),s("p",[t._v("1）combiner是MR程序中Mapper和Reducer之外的一种组件")]),t._v(" "),s("p",[t._v("2）combiner组件的父类就是Reducer")]),t._v(" "),s("p",[t._v("3）combiner和reducer的区别在于运行的位置：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("Combiner是在每一个maptask所在的节点运行")])]),t._v(" "),s("li",[s("p",[t._v("Reducer是接收全局所有Mapper的输出结果；")])])]),t._v(" "),s("p",[t._v("4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量")]),t._v(" "),s("p",[t._v("5）自定义Combiner实现步骤：")]),t._v(" "),s("p",[t._v("（1）自定义一个combiner继承Reducer，重写reduce方法")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordcountCombiner")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("extends")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Reducer")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("protected")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Iterable")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n\t\t\t"),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Context")]),t._v(" context"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("InterruptedException")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" count "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),t._v(" v "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\t\tcount "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t\tcontext"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("write")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IntWritable")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("（2）在job驱动类中设置：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setCombinerClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WordcountCombiner")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n")])])]),s("p",[t._v("6）combiner能够应用的前提是不能影响最终的业务逻辑，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Mapper\n\n3 5 7 ->(3+5+7)/3=5 \n\n2 6 ->(2+6)/2=4\n\nReducer\n\n(3+5+7+2+6)/5=23/5    不等于    (5+4)/2=9/2\n")])])]),s("h3",{attrs:{id:"数据倾斜"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据倾斜","aria-hidden":"true"}},[t._v("#")]),t._v(" 数据倾斜")]),t._v(" "),s("p",[t._v("1）数据倾斜原因")]),t._v(" "),s("p",[t._v("如果是多张表的操作都是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜。")]),t._v(" "),s("p",[t._v("2）解决方案")]),t._v(" "),s("p",[t._v("在map端缓存多张表，提前处理业务逻辑，这样增加map端业务，减少reduce端数据的压力，尽可能的减少数据倾斜。")]),t._v(" "),s("p",[t._v("3）具体办法：采用distributedcache")]),t._v(" "),s("p",[t._v("​\t（1）"),s("strong",[t._v("在mapper的setup阶段，将文件读取到缓存集合中")])]),t._v(" "),s("p",[t._v("​\t（2）在驱动函数中加载缓存。")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("addCacheFile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("URI")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"file:/e:/mapjoincache/pd.txt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 缓存普通文件到task运行节点")]),t._v("\n")])])]),s("h2",{attrs:{id:"reducetask工作机制"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#reducetask工作机制","aria-hidden":"true"}},[t._v("#")]),t._v(" ReduceTask工作机制")]),t._v(" "),s("p",[t._v("1）设置ReduceTask")]),t._v(" "),s("p",[t._v("reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，"),s("strong",[t._v("Reducetask数量的决定是可以直接手动设置")]),t._v("：")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//默认值是1，手动设置为4 ")]),t._v("\njob"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setNumReduceTasks")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n")])])]),s("p",[t._v("2）注意")]),t._v(" "),s("p",[t._v("（1）reducetask=0\t，表示没有reduce阶段，输出文件个数和map个数一致。")]),t._v(" "),s("p",[t._v("（2）reducetask默认值就是1，所以输出文件个数为一个。")]),t._v(" "),s("p",[t._v("（3）如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜")]),t._v(" "),s("p",[t._v("（4）reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask。")]),t._v(" "),s("p",[t._v("（5）具体多少个reducetask，需要根据集群性能而定。")]),t._v(" "),s("p",[t._v("（6）如果分区数不是1，但是reducetask为1，是否执行分区过程。答案是：不执行分区过程。因为在maptask的源码中，执行分区的前提是先判断reduceNum个数是否大于1。不大于1肯定不执行。")]),t._v(" "),s("p",[s("img",{attrs:{src:"http://ww2.sinaimg.cn/large/006tNc79gy1g5acs31epaj31j10pkanu.jpg",alt:"img"}})]),t._v(" "),s("p",[t._v("​\t（1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。")]),t._v(" "),s("p",[t._v("​\t（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。")]),t._v(" "),s("p",[t._v("​\t（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。")]),t._v(" "),s("p",[t._v("​\t（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。")]),t._v(" "),s("h2",{attrs:{id:"数据压缩"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#数据压缩","aria-hidden":"true"}},[t._v("#")]),t._v(" 数据压缩")]),t._v(" "),s("h3",{attrs:{id:"概述"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#概述","aria-hidden":"true"}},[t._v("#")]),t._v(" 概述")]),t._v(" "),s("p",[t._v("压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadood下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。")]),t._v(" "),s("p",[t._v("鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。")]),t._v(" "),s("p",[t._v("如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。")]),t._v(" "),s("p",[t._v("压缩"),s("strong",[t._v("mapreduce")]),t._v("的一种优化策略：通过压缩编码对"),s("strong",[t._v("mapper")]),t._v("或者"),s("strong",[t._v("reducer")]),t._v("的输出进行压缩，以减少磁盘"),s("strong",[t._v("IO")]),t._v("，提高MR程序运行速度（但相应增加了cpu运算负担）。")]),t._v(" "),s("p",[t._v("注意：压缩特性运用得当能提高性能，但运用不当也可能降低性能。")]),t._v(" "),s("p",[t._v("基本原则：")]),t._v(" "),s("p",[t._v("（1）运算密集型的job，少用压缩")]),t._v(" "),s("p",[t._v("（2）IO密集型的job，多用压缩")]),t._v(" "),s("h3",{attrs:{id:"支持的压缩编码"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#支持的压缩编码","aria-hidden":"true"}},[t._v("#")]),t._v(" 支持的压缩编码")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("压缩格式")]),t._v(" "),s("th",[t._v("工具")]),t._v(" "),s("th",[t._v("算法")]),t._v(" "),s("th",[t._v("文件扩展名")]),t._v(" "),s("th",[t._v("是否可切分")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("DEFAULT")]),t._v(" "),s("td",[t._v("无")]),t._v(" "),s("td",[t._v("DEFAULT")]),t._v(" "),s("td",[t._v(".deflate")]),t._v(" "),s("td",[t._v("否")])]),t._v(" "),s("tr",[s("td",[t._v("Gzip")]),t._v(" "),s("td",[t._v("gzip")]),t._v(" "),s("td",[t._v("DEFAULT")]),t._v(" "),s("td",[t._v(".gz")]),t._v(" "),s("td",[t._v("否")])]),t._v(" "),s("tr",[s("td",[t._v("bzip2")]),t._v(" "),s("td",[t._v("bzip2")]),t._v(" "),s("td",[t._v("bzip2")]),t._v(" "),s("td",[t._v(".bz2")]),t._v(" "),s("td",[t._v("是")])]),t._v(" "),s("tr",[s("td",[t._v("LZO")]),t._v(" "),s("td",[t._v("lzop")]),t._v(" "),s("td",[t._v("LZO")]),t._v(" "),s("td",[t._v(".lzo")]),t._v(" "),s("td",[t._v("否")])]),t._v(" "),s("tr",[s("td",[t._v("LZ4")]),t._v(" "),s("td",[t._v("无")]),t._v(" "),s("td",[t._v("LZ4")]),t._v(" "),s("td",[t._v(".lz4")]),t._v(" "),s("td",[t._v("否")])]),t._v(" "),s("tr",[s("td",[t._v("Snappy")]),t._v(" "),s("td",[t._v("无")]),t._v(" "),s("td",[t._v("Snappy")]),t._v(" "),s("td",[t._v(".snappy")]),t._v(" "),s("td",[t._v("否")])])])]),t._v(" "),s("p",[t._v("为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器，如下表所示")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("压缩格式")]),t._v(" "),s("th",[t._v("对应的编码/解码器")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("DEFLATE")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.DefaultCodec")])]),t._v(" "),s("tr",[s("td",[t._v("gzip")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.GzipCodec")])]),t._v(" "),s("tr",[s("td",[t._v("bzip2")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.BZip2Codec")])]),t._v(" "),s("tr",[s("td",[t._v("LZO")]),t._v(" "),s("td",[t._v("com.hadoop.compression.lzo.LzopCodec")])]),t._v(" "),s("tr",[s("td",[t._v("LZ4")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.Lz4Codec")])]),t._v(" "),s("tr",[s("td",[t._v("Snappy")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.SnappyCodec")])])])]),t._v(" "),s("h3",{attrs:{id:"压缩性能的比较"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#压缩性能的比较","aria-hidden":"true"}},[t._v("#")]),t._v(" 压缩性能的比较")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("压缩算法")]),t._v(" "),s("th",[t._v("原始文件大小")]),t._v(" "),s("th",[t._v("压缩文件大小")]),t._v(" "),s("th",[t._v("压缩速度")]),t._v(" "),s("th",[t._v("解压速度")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("gzip")]),t._v(" "),s("td",[t._v("8.3GB")]),t._v(" "),s("td",[t._v("1.8GB")]),t._v(" "),s("td",[t._v("17.5MB/s")]),t._v(" "),s("td",[t._v("58MB/s")])]),t._v(" "),s("tr",[s("td",[t._v("bzip2")]),t._v(" "),s("td",[t._v("8.3GB")]),t._v(" "),s("td",[t._v("1.1GB")]),t._v(" "),s("td",[t._v("2.4MB/s")]),t._v(" "),s("td",[t._v("9.5MB/s")])]),t._v(" "),s("tr",[s("td",[t._v("LZO")]),t._v(" "),s("td",[t._v("8.3GB")]),t._v(" "),s("td",[t._v("2.9GB")]),t._v(" "),s("td",[t._v("49.3MB/s")]),t._v(" "),s("td",[t._v("74.6MB/s")])])])]),t._v(" "),s("p",[t._v("http://google.github.io/snappy/")]),t._v(" "),s("p",[t._v("On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.")]),t._v(" "),s("h3",{attrs:{id:"常见几种压缩算法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常见几种压缩算法","aria-hidden":"true"}},[t._v("#")]),t._v(" 常见几种压缩算法")]),t._v(" "),s("p",[s("strong",[t._v("Gzip压缩")])]),t._v(" "),s("p",[t._v("​    优点：压缩率比较高，而且压缩/解压速度也比较块：hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。")]),t._v(" "),s("p",[t._v("​    缺点：不支持分区（split）")]),t._v(" "),s("p",[t._v("​    应用场景：每当文件压缩之后在130m以内的，都可以考虑用Gzip压缩格式，hive程序，Streaming程序和java写的MapReduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。")]),t._v(" "),s("p",[s("strong",[t._v("Bzip2压缩")])]),t._v(" "),s("p",[t._v("​    优点：支持Split，具有很高的压缩率，比Gzip压缩率都高，hadoop本身支持，但不支持native，在linux系统下自带bzip2命令，使用方便。")]),t._v(" "),s("p",[t._v("​    缺点：压缩、解压速度慢，不支持native")]),t._v(" "),s("p",[t._v("​    应用场景：适合对速度要求不高，但需求较高的压缩率的时候，可以作为MapReduce作业的输出格式，或者输出之后的数据比较大，处理之后的数据需要压缩文档减少磁盘空间并且以后数据用的比较少的情况，或者对单个很大的文本文件想要压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序的情况。")]),t._v(" "),s("p",[s("strong",[t._v("Lzo压缩")])]),t._v(" "),s("p",[t._v("​    优点：压缩/解压速度也比较快，合理的压缩率，支持split，是hadoop中最流行的压缩格式，可以在linux系统下安装lzop命令，使用方便。")]),t._v(" "),s("p",[t._v("​    缺点：压缩比例比gzip要低一些，hadoop本身并不支持，需要安装，在应用中对lzo格式的文件需要做一些特殊处理。（为了支持split需要建立索引，还需要指定inputformat为lzo格式）")]),t._v(" "),s("p",[t._v("​    应用场景：一个很大的文本文件，压缩之后还大于200m以上的可以考虑，而且单个文件越大，lzo优点越明显。")]),t._v(" "),s("p",[s("strong",[t._v("Snappy压缩")])]),t._v(" "),s("p",[t._v("​    优点：高速压缩速度和合理的压缩率")]),t._v(" "),s("p",[t._v("​    缺点：不支持Split，压缩率比gzip低，hadoop本身不支持")]),t._v(" "),s("p",[t._v("​    应用场景：当MapReduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式，或者作为一个MapReduce作业的输出和另外一个MapReduce作业的输入")]),t._v(" "),s("h3",{attrs:{id:"采用压缩的位置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#采用压缩的位置","aria-hidden":"true"}},[t._v("#")]),t._v(" 采用压缩的位置")]),t._v(" "),s("p",[t._v("压缩可以在MapReduce作用的任意阶段启用。")]),t._v(" "),s("p",[t._v("1）输入压缩：")]),t._v(" "),s("p",[t._v("​\t在有大量数据并计划重复处理的情况下，应该考虑对输入进行压缩。然而，你无须显示指定使用的编解码方式。Hadoop自动检查文件扩展名，如果扩展名能够匹配，就会用恰当的编解码方式对文件进行压缩和解压。否则，Hadoop就不会使用任何编解码器。")]),t._v(" "),s("p",[t._v("2）压缩mapper输出：")]),t._v(" "),s("p",[t._v("当map任务输出的中间数据量很大时，应考虑在此阶段采用压缩技术。这能显著改善内部数据Shuffle过程，而Shuffle过程在Hadoop处理过程中是资源消耗最多的环节。如果发现数据量大造成网络传输缓慢，应该考虑使用压缩技术。可用于压缩mapper输出的快速编解码器包括LZO、LZ4或者Snappy。")]),t._v(" "),s("p",[t._v("​\t注：LZO是供Hadoop压缩数据用的通用压缩编解码器。其设计目标是达到与硬盘读取速度相当的压缩速度，因此速度是优先考虑的因素，而不是压缩率。与gzip编解码器相比，它的压缩速度是gzip的5倍，而解压速度是gzip的2倍。同一个文件用LZO压缩后比用gzip压缩后大50%，但比压缩前小25%~50%。这对改善性能非常有利，map阶段完成时间快4倍。")]),t._v(" "),s("p",[t._v("3）压缩reducer输出：")]),t._v(" "),s("p",[t._v("​\t在此阶段启用压缩技术能够减少要存储的数据量，因此降低所需的磁盘空间。当mapreduce作业形成作业链条时，因为第二个作业的输入也已压缩，所以启用压缩同样有效。")]),t._v(" "),s("h3",{attrs:{id:"压缩配置参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#压缩配置参数","aria-hidden":"true"}},[t._v("#")]),t._v(" 压缩配置参数")]),t._v(" "),s("p",[t._v("在Hadoop中启用压缩，可以配置以下参数（mapred-site.xml文件中）：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",[t._v("参数")]),t._v(" "),s("th",[t._v("默认值")]),t._v(" "),s("th",[t._v("阶段")])])]),t._v(" "),s("tbody",[s("tr",[s("td",[t._v("io.compression.ccodecs")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.Lz4Codec,   org.apache.hadoop.io.compress.BZip2Codec,")]),t._v(" "),s("td",[t._v("输入压缩")])]),t._v(" "),s("tr",[s("td",[t._v("mapreduce.map.output.compress")]),t._v(" "),s("td",[t._v("false")]),t._v(" "),s("td",[t._v("mapper输出")])]),t._v(" "),s("tr",[s("td",[t._v("mapreduce.map.output.compress.codec")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.DefaultCodec")]),t._v(" "),s("td",[t._v("mapper输出")])]),t._v(" "),s("tr",[s("td",[t._v("mapreduce.output.fileoutputformat.compress.codec")]),t._v(" "),s("td",[t._v("org.apache.hadoop.io.compress.DefaultCodec")]),t._v(" "),s("td",[t._v("reducer输出")])]),t._v(" "),s("tr",[s("td",[t._v("mapreduce.output.fileoutputformat.compress.type")]),t._v(" "),s("td",[t._v("RECORD")]),t._v(" "),s("td",[t._v("reducer输出")])])])]),t._v(" "),s("h3",{attrs:{id:"压缩、解压缩实例"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#压缩、解压缩实例","aria-hidden":"true"}},[t._v("#")]),t._v(" 压缩、解压缩实例")]),t._v(" "),s("p",[t._v("CompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用createOutputStream（OutputStreamout）方法创建一个CompressionOutputStream,将其以压缩格式写入低层的流。相反，想要对输入流读取而来的数据进行解压缩，则需要调用createInputStream(InputStreamin)函数，从而获取一个CompressionInputStream，从而从低层的流读取未压缩的数据。")]),t._v(" "),s("h2",{attrs:{id:"优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#优化","aria-hidden":"true"}},[t._v("#")]),t._v(" 优化")]),t._v(" "),s("h3",{attrs:{id:"mr跑的慢的原因"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mr跑的慢的原因","aria-hidden":"true"}},[t._v("#")]),t._v(" MR跑的慢的原因")]),t._v(" "),s("p",[t._v("Mapreduce 程序效率的瓶颈在于两点：")]),t._v(" "),s("p",[t._v("1）计算机性能")]),t._v(" "),s("p",[t._v("​\tCPU、内存、磁盘健康、网络")]),t._v(" "),s("p",[t._v("2）I/O 操作优化")]),t._v(" "),s("p",[t._v("（1）数据倾斜")]),t._v(" "),s("p",[t._v("（2）map和reduce数设置不合理")]),t._v(" "),s("p",[t._v("（3）reduce等待过久")]),t._v(" "),s("p",[t._v("（4）小文件过多")]),t._v(" "),s("p",[t._v("（5）大量的不可分块的超大文件")]),t._v(" "),s("p",[t._v("（6）spill次数过多")]),t._v(" "),s("p",[t._v("（7）merge次数过多等。")]),t._v(" "),s("h3",{attrs:{id:"mr优化方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mr优化方法","aria-hidden":"true"}},[t._v("#")]),t._v(" MR优化方法")]),t._v(" "),s("h4",{attrs:{id:"_1）数据输入"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1）数据输入","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("1")]),t._v("）数据输入")]),t._v(" "),s("p",[t._v("（1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致 mr 运行较慢。")]),t._v(" "),s("p",[t._v("（2）采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。")]),t._v(" "),s("p",[t._v("map task的启动数量也和下面这几个参数有关系：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("mapred.min.split.size：Input Split的最小值 默认值1\nmapred.max.split.size：Input Split的最大值\ndfs.block.size：HDFS 中一个block大小，默认值128MB\n")])])]),s("p",[t._v("当mapred.min.split.size小于dfs.block.size的时候，一个block会被分为多个分片，也就是对应多个map task")]),t._v(" "),s("p",[t._v("当mapred.min.split.size大于dfs.block.size的时候，一个分片可能对应多个block，也就是一个map task读取多个block数据")]),t._v(" "),s("p",[t._v("（3）选择合理的Writable类型。为应用程序处理的数据选择合适的Writable类型可大大提升性能。 比如处理整数类型数据时，直接采用IntWritable比先以Text类型读入在转换为整数类型要高效。")]),t._v(" "),s("p",[t._v("（4）增加输入文件的副本数，就是充分利用本地读")]),t._v(" "),s("p",[t._v("（5）集群的网络、IO等性能很好的时候，建议调高block块大小：dfs.block.size。")]),t._v(" "),s("p",[t._v("（6）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。")]),t._v(" "),s("p",[t._v("（7）使用SequenceFile二进制文件")]),t._v(" "),s("h4",{attrs:{id:"_2）map阶段"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2）map阶段","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("2")]),t._v("）"),s("strong",[t._v("map")]),t._v("阶段")]),t._v(" "),s("p",[t._v("（1）减少spill次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘 IO。")]),t._v(" "),s("p",[t._v("（2）减少merge次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。")]),t._v(" "),s("p",[t._v("（3）在 map 之后先进行combine处理，减少 I/O。")]),t._v(" "),s("p",[t._v("我们知道如果map side设置了Combiner，那么会根据设定的函数对map输出的数据进行一次类reduce的预处理")]),t._v(" "),s("p",[t._v("但是和分组、排序分组不一样的是，combine发生的阶段可能是在merge之前，也可能是在merge之后")]),t._v(" "),s("p",[t._v("这个时机可以由一个参数控制：min.num.spill.for.combine，默认值为3")]),t._v(" "),s("p",[t._v("当job中设定了combiner，并且spill数最少有3个的时候，那么combiner函数就会在merge产生结果文件之前运行")]),t._v(" "),s("p",[t._v("例如，产生的spill非常多，虽然我们可以通过merge阶段的io.sort.factor进行优化配置，但是在此之前我们还可以通过先执行combine对结果进行处理之后再对数据进行merge ，这样一来，到merge阶段的数据量将会进一步减少，IO开销也会被降到最低输出中间数据到磁盘")]),t._v(" "),s("p",[t._v("（4）提高map输出的Buffer")]),t._v(" "),s("p",[t._v("该阶段是map side中将结果输出到磁盘之前的一个处理方式，通过对其进行设置的话可以减少map任务的IO开销，从而提高性能。")]),t._v(" "),s("p",[t._v("由于map任务运行时中间结果首先存储在buffer中,默认当缓存的使用量达到80%的时候就开始写入磁盘,这个过程叫做spill(溢出)")]),t._v(" "),s("p",[t._v("这个buffer默认的大小是100M可以通过设定io.sort.mb的值来进行调整")]),t._v(" "),s("p",[t._v("当map产生的数据非常大时，如果默认的buffer大小不够看，那么势必会进行非常多次的spill，进行spill就意味着要写磁盘，产生IO开销 ，这时候就可以把io.sort.mb调大，那么map在整个计算过程中spill的次数就势必会降低，map task对磁盘的操作就会变少")]),t._v(" "),s("p",[t._v("如果map tasks的瓶颈在磁盘上，这样调整就会大大提高map的计算性能，但是如果将io.sort.mb调的非常大的时候，对机器的配置要求就非常高，因为占用内存过大，所以需要根据情况进行配置")]),t._v(" "),s("p",[t._v("map并不是要等到buffer全部写满时才进行spill，因为如果全部写满了再去写spill，势必会造成map的计算部分等待buffer释放空间的情况。")]),t._v(" "),s("p",[t._v("所以，map其实是当buffer被写满到一定程度（比如80%）时，才开始进行spill")]),t._v(" "),s("p",[t._v("可以通过设置io.sort.spill.percent的值来调整这个阈值")]),t._v(" "),s("p",[t._v("这个参数同样也是影响spill频繁程度，进而影响map task运行周期对磁盘的读写频率，但是通常情况下只需要对io.sort.mb进行调整即可")]),t._v(" "),s("p",[t._v("（5）合理的开启map压缩")]),t._v(" "),s("p",[t._v("其实无论是spill的时候，还是最后merge产生的结果文件，都是可以压缩的")]),t._v(" "),s("p",[t._v("压缩的好处在于，通过压缩减少写入读出磁盘的数据量。对中间结果非常大，磁盘速度成为map执行瓶颈的job，尤其有用")]),t._v(" "),s("p",[t._v("控制输出是否使用压缩的参数是mapred.compress.map.output，值为true或者false")]),t._v(" "),s("p",[t._v("启用压缩之后，会牺牲CPU的一些计算资源，但是可以节省IO开销，非常适合IO密集型的作业（如果是CPU密集型的作业不建议设置）")]),t._v(" "),s("p",[t._v("设置压缩的时候，我们可以选择不同的压缩算法")]),t._v(" "),s("p",[t._v("Hadoop默认提供了GzipCodec，LzoCodec，BZip2Codec，LzmaCodec等压缩格式")]),t._v(" "),s("p",[t._v("通常来说，想要达到比较平衡的cpu和磁盘压缩比，LzoCodec比较合适，但也要取决于job的具体情况")]),t._v(" "),s("p",[t._v("如果想要自行选择中间结果的压缩算法，可以设置配置参数：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("//或者其他用户自行选择的压缩方式\nmapred.map.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec\n")])])]),s("p",[t._v("map端调优的相关参数：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("选项\t默认值\t描述\nmapred.min.split.size\t\t1\tInput Split的最小值\nmapred.max.split.size\t\t\tInput Split的最大值\nio.sort.mb\t\t100\tmap缓冲区大小\nio.sort.spill.percent\t\t0.8\t缓冲区阈值\nio.sort.factor\t\t10\t并行处理spill的个数\nmin.num.spill.for.combine\t\t3\t最少有多少个spill的时候combine在merge之前进行\nmapred.compress.map.output\t\tfalse\tmap中间数据是否采用压缩\nmapred.map.output.compression.codec\t\t压缩算法\n")])])]),s("h4",{attrs:{id:"_3）reduce阶段"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3）reduce阶段","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("3")]),t._v("）"),s("strong",[t._v("reduce")]),t._v("阶段")]),t._v(" "),s("p",[t._v("（1）合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。")]),t._v(" "),s("p",[t._v("（2）设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。")]),t._v(" "),s("p",[t._v("（3）Shuffle阶段优化。")]),t._v(" "),s("ul",[s("li",[t._v("设置并行拷贝线程数：mapred.reduce.parallel.copies。")]),t._v(" "),s("li",[t._v("设置merge的buffer大小，减小IO次数")])]),t._v(" "),s("p",[t._v("控制该值的参数为：")]),t._v(" "),s("p",[t._v("mapred.job.shuffle.input.buffer.percent，默认0.7，这是一个百分比，意思是reduce的可用内存中拿出70%作为buffer存放数据")]),t._v(" "),s("p",[t._v("reduce的可用内存通过mapred.child.java.opts来设置，比如置为-Xmx1024m，该参数是同时设定map和reduce task的可用内存，一般为map buffer大小的两倍左右")]),t._v(" "),s("p",[t._v("设置了reduce端的buffer大小，我们同样可以通过一个参数来控制buffer中的数据达到一个阈值的时候开始往磁盘写数据：mapred.job.shuffle.merge.percent，默认为0.66")]),t._v(" "),s("p",[t._v("（4）合理设置reduc端的buffer，默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多个一个写磁盘->读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销：mapred.job.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读buffer中的数据直接拿给reduce使用。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。")]),t._v(" "),s("h4",{attrs:{id:"_4）数据倾斜问题"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4）数据倾斜问题","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("4")]),t._v("）数据倾斜问题")]),t._v(" "),s("p",[t._v("（1）数据倾斜现象")]),t._v(" "),s("p",[t._v("​\t\t数据频率倾斜——某一个区域的数据量要远远大于其他区域。")]),t._v(" "),s("p",[t._v("​\t\t数据大小倾斜——部分记录的大小远远大于平均值。")]),t._v(" "),s("p",[t._v("（2）如何收集倾斜数据")]),t._v(" "),s("p",[t._v("在reduce方法中加入记录map输出键的详细情况的功能。")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("static")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("final")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),t._v(" MAX_VALUES "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"skew.maxvalues"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("private")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" maxValueThreshold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n \n"),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("configure")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JobConf")]),t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" \n     maxValueThreshold "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" job"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getInt")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("MAX_VALUES"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v(" \n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" \n"),s("span",{pre:!0,attrs:{class:"token annotation punctuation"}},[t._v("@Override")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("public")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("void")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("reduce")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Iterator")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                     "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("OutputCollector")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Text")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" output"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                     "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Reporter")]),t._v(" reporter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("throws")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("IOException")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("hasNext")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n         values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("next")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n         i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" maxValueThreshold"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n         log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("info")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Received "')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" values for key "')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" key"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("（3）减少数据倾斜的方法")]),t._v(" "),s("p",[t._v("方法"),s("strong",[t._v("1")]),t._v("：抽样和范围分区。可以通过对原始数据进行抽样得到的结果集来预设分区边界值。")]),t._v(" "),s("p",[t._v("方法"),s("strong",[t._v("2")]),t._v("：自定义分区")]),t._v(" "),s("p",[t._v("另一个抽样和范围分区的替代方案是基于输出键的背景知识进行自定义分区。例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。")]),t._v(" "),s("p",[t._v("方法"),s("strong",[t._v("3")]),t._v("："),s("strong",[t._v("Combine")])]),t._v(" "),s("p",[t._v("使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。在可能的情况下，combine的目的就是聚合并精简数据。")]),t._v(" "),s("p",[t._v("reduce调优主要参数：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("选项\t\t默认值\t描述\nmapred.reduce.parallel.copies\t\t5\t每个reduce去map中拿数据的并行数\nmapred.reduce.copy.backoff\t\t300\t获取map数据最大超时时间\nmapred.job.shuffle.input.buffer.percent\t\t0.7\tbuffer大小占reduce可用内存的比例\nmapred.child.java.opts\t\t-Xmx1024m  设置reduce可用内存为1g\nmapred.job.shuffle.merge.percent\t\t0.66\tbuffer中的数据达到多少比例开始写入磁盘\nmapred.job.reduce.input.buffer.percent\t\t0.0\t指定多少比例的内存用来存放buffer中的数据\n")])])]),s("h3",{attrs:{id:"hdfs小文件优化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hdfs小文件优化","aria-hidden":"true"}},[t._v("#")]),t._v(" "),s("strong",[t._v("HDFS")]),t._v("小文件优化")]),t._v(" "),s("p",[t._v("1）HDFS小文件弊端：")]),t._v(" "),s("p",[t._v("HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。")]),t._v(" "),s("p",[t._v("2）解决的方式：")]),t._v(" "),s("p",[t._v("（1）Hadoop本身提供了一些文件压缩的方案。")]),t._v(" "),s("p",[t._v("（2）从系统层面改变现有HDFS存在的问题，其实主要还是小文件的合并，然后建立比较快速的索引。")]),t._v(" "),s("p",[t._v("3）Hadoop自带小文件解决方案")]),t._v(" "),s("p",[t._v("（1）Hadoop Archive:")]),t._v(" "),s("p",[t._v("​     是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时。")]),t._v(" "),s("p",[t._v("（2）Sequence file：")]),t._v(" "),s("p",[t._v("​     sequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。")]),t._v(" "),s("p",[t._v("（3）CombineFileInputFormat：")]),t._v(" "),s("p",[t._v("​     CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。")]),t._v(" "),s("p",[t._v("4）小文件优化(实战经验)")]),t._v(" "),s("p",[t._v("对于大量小文件Job，可以开启JVM重用会减少45%运行时间。")]),t._v(" "),s("p",[t._v("JVM重用理解：一个map运行一个jvm，重用的话，在一个map在jvm上运行完毕后，jvm继续运行其他jvm")]),t._v(" "),s("p",[t._v("具体设置：mapreduce.job.jvm.numtasks值在10-20之间。")]),t._v(" "),s("h2",{attrs:{id:"面试"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#面试","aria-hidden":"true"}},[t._v("#")]),t._v(" 面试")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("Map 阶段map数量生成原理")]),t._v(" "),s("p",[t._v("取决于文件时和块大小，默认块为128M，将每个文件按照128M切分，算出分区个数")])]),t._v(" "),s("li",[s("p",[t._v("Map阶段map数据读取过程介绍")])]),t._v(" "),s("li",[s("p",[t._v("Map阶段map写数据过程介绍")])]),t._v(" "),s("li",[s("p",[t._v("Reduce阶段shullfe过程介绍")])]),t._v(" "),s("li",[s("p",[t._v("Map阶段可优化参数介绍")])]),t._v(" "),s("li",[s("p",[t._v("Reduce阶段可优化参数介绍")])]),t._v(" "),s("li",[s("p",[t._v("Mapreduce的优点和缺陷")])]),t._v(" "),s("li",[s("p",[t._v("Mapreduce数据倾斜及解决方案")])])])])},[],!1,null,null,null);a.default=n.exports}}]);