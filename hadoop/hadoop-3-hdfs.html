<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>HDFS | JavaChen Wiki</title>
    <meta name="description" content="JavaChen Wiki">
    <link rel="icon" href="/wiki/img/logo.ico">
  <meta http-quiv="pragma" cotent="no-cache">
  <meta http-quiv="pragma" cotent="no-cache,must-revalidate">
  <meta http-quiv="expires" cotent="0">
    
    <link rel="preload" href="/wiki/assets/css/0.styles.bdf32acf.css" as="style"><link rel="preload" href="/wiki/assets/js/app.dd178f5a.js" as="script"><link rel="preload" href="/wiki/assets/js/2.9f273a62.js" as="script"><link rel="preload" href="/wiki/assets/js/22.f6801be0.js" as="script"><link rel="prefetch" href="/wiki/assets/js/10.e389070d.js"><link rel="prefetch" href="/wiki/assets/js/11.19e9f97f.js"><link rel="prefetch" href="/wiki/assets/js/12.f95e0459.js"><link rel="prefetch" href="/wiki/assets/js/13.127730fd.js"><link rel="prefetch" href="/wiki/assets/js/14.34eed2f7.js"><link rel="prefetch" href="/wiki/assets/js/15.a8cf4000.js"><link rel="prefetch" href="/wiki/assets/js/16.6346731c.js"><link rel="prefetch" href="/wiki/assets/js/17.143af375.js"><link rel="prefetch" href="/wiki/assets/js/18.7d95703a.js"><link rel="prefetch" href="/wiki/assets/js/19.8726bd0b.js"><link rel="prefetch" href="/wiki/assets/js/20.2914a04f.js"><link rel="prefetch" href="/wiki/assets/js/21.ebd751e0.js"><link rel="prefetch" href="/wiki/assets/js/23.87ecfe73.js"><link rel="prefetch" href="/wiki/assets/js/24.cabcaf93.js"><link rel="prefetch" href="/wiki/assets/js/25.306828f5.js"><link rel="prefetch" href="/wiki/assets/js/26.59450304.js"><link rel="prefetch" href="/wiki/assets/js/27.44fddb85.js"><link rel="prefetch" href="/wiki/assets/js/28.4e15bea1.js"><link rel="prefetch" href="/wiki/assets/js/29.bed7cbfd.js"><link rel="prefetch" href="/wiki/assets/js/3.28138f41.js"><link rel="prefetch" href="/wiki/assets/js/30.36a67cca.js"><link rel="prefetch" href="/wiki/assets/js/31.dde8666e.js"><link rel="prefetch" href="/wiki/assets/js/32.148ea73a.js"><link rel="prefetch" href="/wiki/assets/js/33.61c937cc.js"><link rel="prefetch" href="/wiki/assets/js/34.eb7c4b0c.js"><link rel="prefetch" href="/wiki/assets/js/35.abb4ce3d.js"><link rel="prefetch" href="/wiki/assets/js/36.f03d0378.js"><link rel="prefetch" href="/wiki/assets/js/37.84491176.js"><link rel="prefetch" href="/wiki/assets/js/38.2a06b872.js"><link rel="prefetch" href="/wiki/assets/js/39.63965d3b.js"><link rel="prefetch" href="/wiki/assets/js/4.c0af9872.js"><link rel="prefetch" href="/wiki/assets/js/40.bf2e354a.js"><link rel="prefetch" href="/wiki/assets/js/41.bed53451.js"><link rel="prefetch" href="/wiki/assets/js/42.34d4f2f5.js"><link rel="prefetch" href="/wiki/assets/js/43.867c0e23.js"><link rel="prefetch" href="/wiki/assets/js/44.0201a314.js"><link rel="prefetch" href="/wiki/assets/js/45.d6f1a311.js"><link rel="prefetch" href="/wiki/assets/js/46.419d44e4.js"><link rel="prefetch" href="/wiki/assets/js/47.a3f8b595.js"><link rel="prefetch" href="/wiki/assets/js/48.574cc219.js"><link rel="prefetch" href="/wiki/assets/js/49.224dd6c4.js"><link rel="prefetch" href="/wiki/assets/js/5.c521f8b1.js"><link rel="prefetch" href="/wiki/assets/js/50.c8825040.js"><link rel="prefetch" href="/wiki/assets/js/51.2dab614a.js"><link rel="prefetch" href="/wiki/assets/js/52.8382a180.js"><link rel="prefetch" href="/wiki/assets/js/53.bfab18a9.js"><link rel="prefetch" href="/wiki/assets/js/54.87a1d37c.js"><link rel="prefetch" href="/wiki/assets/js/55.5f207671.js"><link rel="prefetch" href="/wiki/assets/js/56.f0e8e082.js"><link rel="prefetch" href="/wiki/assets/js/57.c868e5c9.js"><link rel="prefetch" href="/wiki/assets/js/6.6a737c5b.js"><link rel="prefetch" href="/wiki/assets/js/7.48bba6b4.js"><link rel="prefetch" href="/wiki/assets/js/8.26532964.js"><link rel="prefetch" href="/wiki/assets/js/9.3c4a4032.js">
    <link rel="stylesheet" href="/wiki/assets/css/0.styles.bdf32acf.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/wiki/" class="home-link router-link-active"><!----> <span class="site-name">JavaChen Wiki</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/wiki/" class="nav-link">主页</a></div><div class="nav-item"><a href="/wiki/docker/" class="nav-link">Docker</a></div><div class="nav-item"><a href="/wiki/elasticsearch/" class="nav-link">Elasticsearch</a></div><div class="nav-item"><a href="/wiki/hadoop/" class="nav-link router-link-active">Hadoop</a></div><div class="nav-item"><a href="/wiki/interview/" class="nav-link">Interview</a></div><div class="nav-item"><a href="/wiki/rabbitMQ/" class="nav-link">RabbitMQ</a></div><div class="nav-item"><a href="/wiki/redis/" class="nav-link">Redis</a></div><div class="nav-item"><a href="/wiki/spring-cloud-cshop/" class="nav-link">Spring-cloud-cshop</a></div><div class="nav-item"><a href="/wiki/about.html" class="nav-link">关于</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/wiki/" class="nav-link">主页</a></div><div class="nav-item"><a href="/wiki/docker/" class="nav-link">Docker</a></div><div class="nav-item"><a href="/wiki/elasticsearch/" class="nav-link">Elasticsearch</a></div><div class="nav-item"><a href="/wiki/hadoop/" class="nav-link router-link-active">Hadoop</a></div><div class="nav-item"><a href="/wiki/interview/" class="nav-link">Interview</a></div><div class="nav-item"><a href="/wiki/rabbitMQ/" class="nav-link">RabbitMQ</a></div><div class="nav-item"><a href="/wiki/redis/" class="nav-link">Redis</a></div><div class="nav-item"><a href="/wiki/spring-cloud-cshop/" class="nav-link">Spring-cloud-cshop</a></div><div class="nav-item"><a href="/wiki/about.html" class="nav-link">关于</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Hadoop</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/wiki/hadoop/" class="sidebar-link">大数据概论</a></li><li><a href="/wiki/hadoop/hadoop-2-Hadoop概述.html" class="sidebar-link">Hadoop</a></li><li><a href="/wiki/hadoop/hadoop-3-hdfs.html" class="active sidebar-link">HDFS</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#概念" class="sidebar-link">概念</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#架构" class="sidebar-link">架构</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs文件块大小" class="sidebar-link">HDFS文件块大小</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs命令" class="sidebar-link">HDFS命令</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs写过程" class="sidebar-link">HDFS写过程</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#网络拓扑" class="sidebar-link">网络拓扑</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#机架感知" class="sidebar-link">机架感知</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs读过程" class="sidebar-link">HDFS读过程</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#namenode工作机制" class="sidebar-link">NameNode工作机制</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#集群安全模式" class="sidebar-link">集群安全模式</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#datanode工作机制" class="sidebar-link">DataNode工作机制</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#集群间数据拷贝" class="sidebar-link">集群间数据拷贝</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hadoop存档" class="sidebar-link">Hadoop存档</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#快照管理" class="sidebar-link">快照管理</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#回收站" class="sidebar-link">回收站</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs-ha高可用" class="sidebar-link">HDFS HA高可用</a></li><li class="sidebar-sub-header"><a href="/wiki/hadoop/hadoop-3-hdfs.html#hdfs-federation架构设计" class="sidebar-link">HDFS Federation架构设计</a></li></ul></li><li><a href="/wiki/hadoop/hadoop-4-MapReduce.html" class="sidebar-link">MapReduce</a></li><li><a href="/wiki/hadoop/hadoop-5-Yarn.html" class="sidebar-link">Yarn</a></li><li><a href="/wiki/hadoop/hadoop-6-Hive.html" class="sidebar-link">Hive</a></li><li><a href="/wiki/hadoop/hadoop-7-Kafka.html" class="sidebar-link">Kafka</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="hdfs"><a href="#hdfs" aria-hidden="true" class="header-anchor">#</a> HDFS</h1> <h2 id="概念"><a href="#概念" aria-hidden="true" class="header-anchor">#</a> 概念</h2> <p><strong>HDFS</strong>，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。</p> <p>HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。</p> <h2 id="架构"><a href="#架构" aria-hidden="true" class="header-anchor">#</a> 架构</h2> <p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g5aasb94mhj30ow0hedkn.jpg" alt="img"></p> <p>HDFS集群包括，NameNode和DataNode以及Secondary Namenode。</p> <p>NameNode：基于内存存储，不会和磁盘发生交换：只存在内存；持久化，写日志文件。</p> <p>NameNode持久化：</p> <ul><li>元数据启动后加载到内存</li> <li>元数据存储到磁盘文件名为fsimage</li> <li>block的位置信息不回保存到fsimage</li> <li>edits记录对元数据的操作</li></ul> <p>NameNode作用：</p> <ul><li>接收客户端读写服务</li> <li>收集dn汇报的block列表信息</li> <li>保存元数据：
<ul><li>文件ownership和permission、大小、时间</li> <li>block列表：偏移量、位置信息</li> <li>block每个副本位置（由dn上报）</li></ul></li></ul> <p>DataNode：</p> <ul><li>本地磁盘目录存储数据（block），文件形式</li> <li>同是存储block的元数据信息文件、MD5</li> <li>启动dn时向nn汇报block信息</li> <li>通过向nn发送心跳保持与其联系（3秒），如果nn10分钟没有收到dn的心跳，则认为已经lost，并copy其上的block到其他dn</li></ul> <p>Secondary NameNode：</p> <ul><li>不是nn备份，但可以做备份，主要帮助nn合并edits log到fsimage，减少nn启动时间</li> <li>执行合并时机：
<ul><li>根据配置文件：fs.checkpoint.period，默认3600秒</li> <li>根据edits log大小，fs.checkpoint.size，默认64M</li></ul></li></ul> <h2 id="hdfs文件块大小"><a href="#hdfs文件块大小" aria-hidden="true" class="header-anchor">#</a> HDFS文件块大小</h2> <p>HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。<strong>可以减少元数据量，有利于顺讯读写。</strong></p> <p>HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。</p> <p>可靠性：</p> <ul><li>数据以副本形式保存在多个节点上，3个副本</li> <li>考虑机架信息</li></ul> <h2 id="hdfs命令"><a href="#hdfs命令" aria-hidden="true" class="header-anchor">#</a> HDFS命令</h2> <p>基本语法：</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs
</code></pre></div><p>常用命令实操</p> <p>（1）-help：输出这个命令参数</p> <div class="language- extra-class"><pre class="language-text"><code>bin/hdfs dfs -help rm
</code></pre></div><p>（2）-ls: 显示目录信息</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -ls /
</code></pre></div><p>（3）-mkdir：在hdfs上创建目录</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd
</code></pre></div><p>（4）-moveFromLocal从本地剪切粘贴到hdfs</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  - moveFromLocal  /home/hadoop/a.txt  /aaa/bbb/cc/dd
</code></pre></div><p>（5）-moveToLocal：从hdfs剪切粘贴到本地</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  - moveToLocal  /aaa/bbb/cc/dd /home/hadoop/a.txt  
</code></pre></div><p>（6）--appendToFile  ：追加一个文件到已经存在的文件末尾</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -appendToFile  ./hello.txt  /hello.txt
</code></pre></div><p>（7）-cat ：显示文件内容</p> <p>（8）-tail：显示一个文件的末尾</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -tail  /weblog/access_log
</code></pre></div><p>（9）-chgrp 、-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -chmod  666  /hello.txt

hadoop  fs  -chown  someuser:somegrp   /hello.txt
</code></pre></div><p>（10）-copyFromLocal：从本地文件系统中拷贝文件到hdfs路径去</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/
</code></pre></div><p>（11）-copyToLocal：从hdfs拷贝到本地</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -copyToLocal /user/hello.txt ./hello.txt
</code></pre></div><p>（12）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2
</code></pre></div><p>（13）-mv：在hdfs目录中移动文件</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -mv  /aaa/jdk.tar.gz  /
</code></pre></div><p>（14）-get：等同于copyToLocal，就是从hdfs下载文件到本地</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -get /user/hello.txt ./
</code></pre></div><p>（15）-getmerge  ：合并下载多个文件，比如hdfs的目录 /aaa/下有多个文件:log.1, <em>log.2,log.3,...</em></p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -getmerge /aaa/log.* ./log.sum
</code></pre></div><p>（16）-put：等同于copyFromLocal</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2
</code></pre></div><p>（17）-rm：删除文件或文件夹</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -rm -r /aaa/bbb/
</code></pre></div><p>（18）-rmdir：删除空目录</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -rmdir   /aaa/bbb/ccc
</code></pre></div><p>（19）-df ：统计文件系统的可用空间信息</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop  fs  -df  -h  /
</code></pre></div><p>（20）-du统计文件夹的大小信息</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -du -s -h /user/wcinput

188.5 M  /user/wcinput
</code></pre></div><p>（21）-count：统计一个指定目录下的文件节点数量</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -count /aaa/
</code></pre></div><p>（22）-setrep：设置hdfs中文件的副本数量</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -setrep 3 /aaa/jdk.tar.gz
</code></pre></div><p>这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。</p> <h2 id="hdfs写过程"><a href="#hdfs写过程" aria-hidden="true" class="header-anchor">#</a> HDFS写过程</h2> <p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g5aasje7hqj31ia0ohqi0.jpg" alt="img"></p> <p>1）客户端向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。</p> <p>2）namenode返回是否可以上传。</p> <p>3）客户端请求第一个 block上传到哪几个datanode服务器上。</p> <p>4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。</p> <p>5）客户端请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成</p> <p>6）dn1、dn2、dn3逐级应答客户端</p> <p>7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答</p> <p>8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）</p> <h2 id="网络拓扑"><a href="#网络拓扑" aria-hidden="true" class="header-anchor">#</a> 网络拓扑</h2> <p>在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。</p> <p>节点距离：两个节点到达最近的共同祖先的距离总和。</p> <p>例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。</p> <p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5aasljratj31hl0pgjze.jpg" alt="img"></p> <div class="language- extra-class"><pre class="language-text"><code>Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程）

Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）

Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点）

Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点）
</code></pre></div><h2 id="机架感知"><a href="#机架感知" aria-hidden="true" class="header-anchor">#</a> 机架感知</h2> <p>低版本Hadoop副本节点选择</p> <ul><li><p>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。</p></li> <li><p>第二个副本和第一个副本位于不相同机架的随机节点上。</p></li> <li><p>第三个副本和第二个副本位于相同机架，节点随机。</p></li></ul> <p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g5aasnddcwj30iy0ehaba.jpg" alt="img"></p> <p>Hadoop2.7.2副本节点选择</p> <ul><li>第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。</li> <li>第二个副本和第一个副本位于相同机架，随机节点。</li> <li>第三个副本位于不同机架，随机节点。</li></ul> <p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g5aasorzbcj30he0d8t9u.jpg" alt="img"></p> <h2 id="hdfs读过程"><a href="#hdfs读过程" aria-hidden="true" class="header-anchor">#</a> HDFS读过程</h2> <p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5fqw4f3vzj30ii08fabc.jpg" alt="img"></p> <p>1）客户端向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。</p> <p>2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。</p> <p>3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。</p> <p>4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。</p> <h2 id="namenode工作机制"><a href="#namenode工作机制" aria-hidden="true" class="header-anchor">#</a> NameNode工作机制</h2> <h3 id="namenode工作机制-2"><a href="#namenode工作机制-2" aria-hidden="true" class="header-anchor">#</a> NameNode工作机制</h3> <p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g5aaswd67pj31i20qa43f.jpg" alt="img"></p> <p>1）第一阶段：namenode启动</p> <p>（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p> <p>（2）客户端对元数据进行增删改的请求</p> <p>（3）namenode记录操作日志，更新滚动日志。</p> <p>（4）namenode在内存中对数据进行增删改查</p> <p>2）第二阶段：Secondary NameNode工作</p> <p>（1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。</p> <p>（2）Secondary NameNode请求执行checkpoint。</p> <p>（3）namenode滚动正在写的edits日志</p> <p>（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode</p> <p>（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p> <p>（6）生成新的镜像文件fsimage.chkpoint</p> <p>（7）拷贝fsimage.chkpoint到namenode</p> <p>（8）namenode将fsimage.chkpoint重新命名成fsimage</p> <p>3）web端访问SecondaryNameNode</p> <p>（1）启动集群</p> <p>（2）浏览器中输入：http://ip:50090/status.html</p> <p>（3）查看SecondaryNameNode信息</p> <h3 id="镜像文件和编辑日志文件"><a href="#镜像文件和编辑日志文件" aria-hidden="true" class="header-anchor">#</a> 镜像文件和编辑日志文件</h3> <p>1）概念</p> <p>namenode被格式化之后，将在/var/hadoop/dfs/name/current目录中产生如下文件</p> <div class="language- extra-class"><pre class="language-text"><code>edits_0000000000000000001-0000000000000000001  fsimage_0000000000000000000      seen_txid
edits_inprogress_0000000000000000002           fsimage_0000000000000000000.md5  VERSION
</code></pre></div><p>（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件idnode的序列化信息。</p> <p>（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到edits文件中。</p> <p>（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字</p> <p>（4）每次Namenode启动的时候都会将fsimage文件读入内存，并从00001开始到seen_txid中记录的数字依次执行每个edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成Namenode启动的时候就将fsimage和edits文件进行了合并。</p> <p>2）oiv查看fsimage文件</p> <p>oiv基本语法</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径
</code></pre></div><p>案例实操</p> <div class="language-bash extra-class"><pre class="language-bash"><code>$ <span class="token function">pwd</span>
/var/hadoop/dfs/name/current

$ hdfs oiv -p XML -i fsimage_0000000000000000000 -o ~/fsimage.xml 

<span class="token function">cat</span> ~/fsimage.xml 
</code></pre></div><p>将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。</p> <p>3）oev查看edits文件</p> <p>基本语法</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径
</code></pre></div><p>案例实操</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs oev -p XML -i edits_0000000000000000001-0000000000000000001 -o ~/edits.xml

cat ~/edits.xml
</code></pre></div><p>将显示的xml文件内容拷贝到eclipse中创建的xml文件中，并格式化。</p> <h3 id="滚动编辑日志"><a href="#滚动编辑日志" aria-hidden="true" class="header-anchor">#</a> 滚动编辑日志</h3> <p>正常情况HDFS文件系统有更新操作时，就会滚动编辑日志。也可以用命令强制滚动编辑日志。</p> <p>1）滚动编辑日志（前提必须启动集群）</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs dfsadmin -rollEdits
</code></pre></div><p>2）镜像文件什么时候产生</p> <p>Namenode启动时加载镜像文件和编辑日志</p> <h3 id="namenode版本号"><a href="#namenode版本号" aria-hidden="true" class="header-anchor">#</a> namenode版本号</h3> <p>1）查看namenode版本号</p> <div class="language-BASH extra-class"><pre class="language-bash"><code><span class="token punctuation">[</span>root@cdh1 current<span class="token punctuation">]</span><span class="token comment"># cat VERSION</span>
<span class="token comment">#Wed Jul 03 15:16:19 CST 2019</span>
namespaceID<span class="token operator">=</span>1472453163
clusterID<span class="token operator">=</span>CID-62f19cc9-8e74-43ac-877f-fdce17ba3c1c
cTime<span class="token operator">=</span>1562138179913
storageType<span class="token operator">=</span>NAME_NODE
blockpoolID<span class="token operator">=</span>BP-126619393-192.168.56.11-1562138179913
layoutVersion<span class="token operator">=</span>-64
</code></pre></div><p>2）namenode版本号具体解释</p> <p>（1） namespaceID在HDFS上，会有多个Namenode，所以不同Namenode的namespaceID是不同的，分别管理一组blockpoolID。</p> <p>（2）clusterID集群id，全局唯一</p> <p>（3）cTime属性标记了namenode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p> <p>（4）storageType属性说明该存储目录包含的是namenode的数据结构。</p> <p>（5）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。</p> <p>（6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本</p> <h2 id="集群安全模式"><a href="#集群安全模式" aria-hidden="true" class="header-anchor">#</a> 集群安全模式</h2> <p>1）概述</p> <p>Namenode启动时，首先将映像文件（fsimage）载入内存，并执行编辑日志（edits）中的各项操作。一旦在内存中成功建立文件系统元数据的映像，则创建一个空的fsimage文件和一个空的编辑日志。由SNN执行合并，返回nn fsimage.chpt并创建一个edits.new。此时，namenode运行在安全模式，即namenode的文件系统对于客户端来说是只读的。</p> <p>系统中的数据块的位置并不是由namenode维护的，而是以块列表的形式存储在datanode中。在系统的正常操作期间，namenode会在内存中保留所有块位置的映射信息。在安全模式下，各个datanode会向namenode发送最新的块列表信息，namenode了解到足够多的块位置信息之后，即可高效运行文件系统。</p> <p>如果满足“最小副本条件”，namenode会在30秒钟之后就退出安全模式。所谓的最小副本条件指的是在整个文件系统中99.9%的块满足最小副本级别（默认值：dfs.replication.min=1）。在启动一个刚刚格式化的HDFS集群时，因为系统中还没有任何块，所以namenode不会进入安全模式。</p> <p>2）基本语法</p> <p>集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。</p> <div class="language-BASH extra-class"><pre class="language-bash"><code>bin/hdfs dfsadmin -safemode get		<span class="token comment">#查看安全模式状态</span>

bin/hdfs dfsadmin -safemode enter  	<span class="token comment">#功能描述：进入安全模式状态 </span>

bin/hdfs dfsadmin -safemode leave	<span class="token comment">#功能描述：离开安全模式状态 </span>

bin/hdfs dfsadmin -safemode <span class="token function">wait</span>	<span class="token comment">#功能描述：等待安全模式状态 </span>
</code></pre></div><p>3）案例</p> <p>模拟等待安全模式</p> <p>1）先进入安全模式</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs dfsadmin -safemode enter
</code></pre></div><p>2）执行下面的脚本</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token shebang important">#!/bin/bash</span>
hdfs dfsadmin -safemode <span class="token function">wait</span>
hdfs dfs -put ~/hello.txt /root/hello.txt
</code></pre></div><p>3）退出安全模式</p> <div class="language-bash extra-class"><pre class="language-bash"><code>hdfs dfsadmin -safemode leave
</code></pre></div><h2 id="datanode工作机制"><a href="#datanode工作机制" aria-hidden="true" class="header-anchor">#</a> DataNode工作机制</h2> <h3 id="datanode工作机制-2"><a href="#datanode工作机制-2" aria-hidden="true" class="header-anchor">#</a> DataNode工作机制</h3> <p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g5abfw0vduj31h60oy16x.jpg" alt="img"></p> <p>1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p> <p>2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。</p> <p>3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。</p> <p>4）集群运行中可以安全加入和退出一些机器</p> <h3 id="数据完整性"><a href="#数据完整性" aria-hidden="true" class="header-anchor">#</a> 数据完整性</h3> <p>1）当DataNode读取block的时候，它会计算checksum</p> <p>2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。</p> <p>3）client读取其他DataNode上的block.</p> <p>4）datanode在其文件创建后周期验证checksum</p> <h3 id="掉线时限参数设置"><a href="#掉线时限参数设置" aria-hidden="true" class="header-anchor">#</a> 掉线时限参数设置</h3> <p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p> <div class="language- extra-class"><pre class="language-text"><code>timeout  = 2 * dfs.namenode.heartbeat.recheck-interval + 10 * dfs.heartbeat.interval
</code></pre></div><p>而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。</p> <p>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。</p> <div class="language-xml extra-class"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.namenode.heartbeat.recheck-interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>300000<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>dfs.heartbeat.interval <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre></div><h3 id="datanode的目录结构"><a href="#datanode的目录结构" aria-hidden="true" class="header-anchor">#</a> DataNode的目录结构</h3> <p>和namenode不同的是，datanode的存储目录是初始阶段自动创建的，不需要额外格式化。</p> <p>1）在/var/hadoop/dfs/data/current这个目录下查看版本号</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token punctuation">[</span>root@cdh1 current<span class="token punctuation">]</span><span class="token comment"># cat VERSION</span>
<span class="token comment">#Wed Jul 03 15:45:10 CST 2019</span>
storageID<span class="token operator">=</span>DS-da8ac7d0-1905-4851-9071-535d3b5d99a6
clusterID<span class="token operator">=</span>CID-62f19cc9-8e74-43ac-877f-fdce17ba3c1c
cTime<span class="token operator">=</span>0
datanodeUuid<span class="token operator">=</span>bdbac640-57fc-4f25-a539-49aceaf333be
storageType<span class="token operator">=</span>DATA_NODE
layoutVersion<span class="token operator">=</span>-57
</code></pre></div><p>2）具体解释</p> <p>（1）storageID：存储id号</p> <p>（2）clusterID集群id，全局唯一</p> <p>（3）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p> <p>（4）datanodeUuid：datanode的唯一识别码</p> <p>（5）storageType：存储类型</p> <p>（6）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。</p> <p>3）在/var/hadoop/dfs/data/current/BP-126619393-192.168.56.11-1562138179913/current这个目录下查看该数据块的版本号</p> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token punctuation">[</span>root@cdh1 current<span class="token punctuation">]</span><span class="token comment"># cat VERSION</span>
<span class="token comment">#Wed Jul 03 15:45:10 CST 2019</span>
namespaceID<span class="token operator">=</span>1472453163
cTime<span class="token operator">=</span>1562138179913
blockpoolID<span class="token operator">=</span>BP-126619393-192.168.56.11-1562138179913
layoutVersion<span class="token operator">=</span>-57
</code></pre></div><p>4）具体解释</p> <p>（1）namespaceID：是datanode首次访问namenode的时候从namenode处获取的storageID对每个datanode来说是唯一的（但对于单个datanode中所有存储目录来说则是相同的），namenode可用这个属性来区分不同datanode。</p> <p>（2）cTime属性标记了datanode存储系统的创建时间，对于刚刚格式化的存储系统，这个属性为0；但是在文件系统升级之后，该值会更新到新的时间戳。</p> <p>（3）blockpoolID：一个block pool id标识一个block pool，并且是跨集群的全局唯一。当一个新的Namespace被创建的时候(format过程的一部分)会创建并持久化一个唯一ID。在创建过程构建全局唯一的BlockPoolID比人为的配置更可靠一些。NN将BlockPoolID持久化到磁盘中，在后续的启动过程中，会再次load并使用。</p> <p>（4）layoutVersion是一个负整数。通常只有HDFS增加新特性时才会更新这个版本号。</p> <h2 id="集群间数据拷贝"><a href="#集群间数据拷贝" aria-hidden="true" class="header-anchor">#</a> 集群间数据拷贝</h2> <p>1）scp实现两个远程主机之间的文件复制</p> <div class="language- extra-class"><pre class="language-text"><code>scp -r hello.txt root@cdh1:/user/root/hello.txt	// 推push

scp -r root@cdh1:/user/root/hello.txt  hello.txt	//拉pull

//是通过本地主机中转实现两个远程主机的文件复制；如果在两个远程主机之间ssh没有配置的情况下可以使用该方式。
scp -r root@cdh1:/user/root/hello.txt root@cdh2:/user/root   
</code></pre></div><p>2）采用discp命令实现两个hadoop集群之间的递归数据复制</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop distcp hdfs://cdh1:8200/user/root/hello.txt hdfs://idh1:8200/user/root/hello.txt
</code></pre></div><h3 id="distcp"><a href="#distcp" aria-hidden="true" class="header-anchor">#</a> DistCp</h3> <p>DistCp（Distributed Copy）是用于大规模集群内部或者集群之间的高性能拷贝工具。 它使用Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。 它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。</p> <p>DistCp是Apache Hadoop自带的工具，目前存在两个版本，DistCp1和DistCp2，FastCopy是Facebook Hadoop中自带的，相比于Distcp，它能明显加快同节点数据拷贝速度，尤其是Hadoop 2.0稳定版（第一个稳定版为2.2.0，该版本包含的特性可参考我的这篇文章：Hadoop 2.0稳定版本2.2.0新特性剖析）发布后，当需要在不同NameNode间（HDFS Federation）迁移数据时，FastCopy将发挥它的最大用武之地。</p> <p>DistCp第一版使用了MapReduce并发拷贝数据，它将整个数据拷贝过程转化为一个map-only Job以加快拷贝速度。由于DistCp本质上是一个MapReduce作业，它需要保证文件中各个block的有序性，因此它的最小数据切分粒度是文件，也就是说，一个文件不能被切分成不同部分让多个任务并行拷贝，最小只能做到一个文件交给一个任务。</p> <p>DistCp2针对DistCp1在易用性和性能等方面的不足，提出了一系列改进点，包括通过去掉不必要的检查缩短了目录扫描时间、动态分配各个Map Task的数据量、可对拷贝限速避免占用过多网络流量、支持HSFTP等。尤其值得一说的是动态分配Map Task处理数据量。DistCp1的实现跟我们平时写的大部分MapReduce程序一样，每个Map Task的待处理数据量在作业开始运行前已经静态分配好了，这就出现了我们经常看到的拖后腿的现象：由于一个Map Task分配的数据量过多，运行非常缓慢，所有Reduce Task都在等待这个Map Task运行完成。而对于DistCp而言，该现象更加常见，因为最小的数据划分单位是文件，文件有大有小，分到大文件的Map Task将运行的非常慢，比如你有两个待拷贝的文件，一个大小为1GB，另一个大小为1TB，如果你指定了超过2个的Map Task，则该DistCp只会启动两个Map Task，其中一个负责拷贝1GB的文件，另一个负责拷贝1TB的文件，可以想象其中一个任务将运行的非常慢。DistCp2通过动态分配Map Task数据量解决了该问题，它实现了一个DynamicInputFormat，该InputFormat将待拷贝的目录文件分解成很多的chunk，其中每个chunk的信息（位置，文件名等）写到一个以“.chunk.K”（K是一个数字）结尾的HDFS文件中，这样，每个文件可看做一份“任务”，“任务”数目要远大于启动的Map Task数目，运行快的Map Task能够多领取一些“任务”，而运行慢得则领取少一些，进而提高数据拷贝速度。尽管DistCp1中Map Task拷贝数据最小单位仍是文件，但相比于DistCp1，则要高效得多，尤其是在文件数据庞大，且大小差距较大的情况下。</p> <p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5abg0c0g1j30b408pmzi.jpg" alt="image-20190705160946607"></p> <p>不管是DistCp1还是DistCp2，在数据拷贝过程中均存在数据低效问题，尤其在Hadoop 2.0时代表现突出。Hadoop 2.0引入了HDFS Federation（什么是HDFS Federation，可参考：HDFS Federation设计动机与基本原理），当我们进行Hadoop（1.0升级到2.0）升级或者将一个NameNode扩展到多个NameNode时，需将集群中的单个NameNode上的部分数据迁移到其他NameNode上，此时就需要用到DistCp这样的工具。在HDFS Federation设计中，一个HDFS集群中可以有多个NameNode，但DataNode是共享的，因此，在数据迁移过程中，大部分数据所在的节点不会变（在同一个DataNode上），只需将其指向新的NameNode（即数据位置不变，元数据转移到其他NameNode上）。如果使用DistCp，则需要将数据重新通过网络拷贝一份，然后将旧的删除，性能十分低下。考虑到数据仍在同一个节点上，则采用文件硬链接.</p> <h3 id="fastcopy"><a href="#fastcopy" aria-hidden="true" class="header-anchor">#</a> FastCopy</h3> <p>基于linux系统ln 本地创建文件的硬链接方式，提升拷贝速度。流程如下：</p> <p>1、查询源NS中文件的meta信息，获取源文件所有的block信息</p> <p>2、对于每个Block，获取其在原集群中的location信息</p> <p>3、对于源文件中的每个block，在目标NS中的文件上添加空的block信息</p> <p>4、对于所有的源Block，通过DN的copyBlock接口实现local copy</p> <p>5、每个目标DN在完成block的copy之后向目标NS的NN中报告接收的Block</p> <p>6、等待所有的block都copy完成后推出 基本结构如下：</p> <p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g5abg41p3aj30bx05i0tb.jpg" alt="pastedGraphic.png"></p> <p>详细介绍</p> <p>1、File Meta复制</p> <p>FastCopy首先获取源文件的meta信息(FileStatus)和blocks locations(LocatedBlocks)信息</p> <p>检查源文件是否处于构建中，LocatedBlocks.isUnderConstruction()，如果是则跳过该文件</p> <p>在目标NS中创建目标文件， 副本数，permission， blockSize等信息与源文件一致，为避免目标文件已存在，默认使用覆写模式创建</p> <p>2、Block 复制</p> <p>对于源文件的LocatedBlocks中所有的block信息，进行Block复制：</p> <p>通过向目标NS中addBlock获取目标NS中block的DN 列表</p> <p>对源block的DN列表和目标Block的DN列表进行排序对齐，使相同的DN在各自的列表中的位置相同</p> <p>通过源Datanode的copyBlock()接口实现想目标DN的block数据复制DataNode.copyBlock()的具体实现分为三种情况：</p> <p>a、同一DN实例</p> <p>这种情况存在于Federation中同一DN节点服务于两个NS的情况，事实上只需要为源Block的文件创建一个HardLink指向目标block</p> <p>b、同一节点上不通DN实例</p> <p>（云梯中暂时不存在这种单节点多DN实例的情况） 这种情况实际上文件位于同一台物理节点上，也可以通过HardLink完成，但由于两个DN实例维护不不同的VolumeMap，因此，需要源DN实例调用目标DN的copyBlockLocal（）接口实现，copyBlockLocal本质上也是使用HardLink来完成copy</p> <p>c、不同DN节点</p> <p>通常源DN和目标DN都是位于不同节点上的，需要通过网络传输block数据，这个传输过程与client向DN写入Block数据基本一致，因此可以直接使用DataTransfer来完成。</p> <p>3、Lease更新</p> <p>由于FastCopy在复制过程中需要对目标文件进行写入，但不是使用FileSystem的API（HDFS默认的lease机制是位于DFSClient中），因此FastCopy需要自己完成Lease的更新。对于每个copy的文件，FastCopy都需要启用一个LeaseChecker线程定期更新lease，保证数据写入的一致性。</p> <p>4、复制状态监控</p> <p>批量的文件copy是异步执行的，FastCopy内部通过一个fileStatusMap维护所有需要复制的文件，文件的状态中包括文件名，文件的block数以及已经完成复制的block， 以及一个blocksStatusMap维护每个需要复制的block的状态，block的状态包括block的副本总数，已经写入成功的副本数，以及写入失败的副本数。</p> <h2 id="hadoop存档"><a href="#hadoop存档" aria-hidden="true" class="header-anchor">#</a> Hadoop存档</h2> <p>每个文件均按块存储，每个块的元数据存储在namenode的内存中，因此hadoop存储小文件会非常低效。因为大量的小文件会耗尽namenode中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件原始内容所需要的磁盘空间相比也不会增多。例如，一个1MB的文件以大小为128MB的块存储，使用的是1MB的磁盘空间，而不是128MB。</p> <p>Hadoop存档文件或HAR文件，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少namenode内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作MapReduce的输入。</p> <h2 id="快照管理"><a href="#快照管理" aria-hidden="true" class="header-anchor">#</a> 快照管理</h2> <p>快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。</p> <p>1）基本语法</p> <p>（1）hdfs dfsadmin -allowSnapshot 路径   （功能描述：开启指定目录的快照功能）</p> <p>（2）hdfs dfsadmin -disallowSnapshot 路径 （功能描述：禁用指定目录的快照功能，默认是禁用）</p> <p>（3）hdfs dfs -createSnapshot 路径        （功能描述：对目录创建快照）</p> <p>（4）hdfs dfs -createSnapshot 路径 名称   （功能描述：指定名称创建快照）</p> <p>（5）hdfs dfs -renameSnapshot 路径 旧名称 新名称 （功能描述：重命名快照）</p> <p>（6）hdfs lsSnapshottableDir         （功能描述：列出当前用户所有可快照目录）</p> <p>（7）hdfs snapshotDiff 路径1 路径2 （功能描述：比较两个快照目录的不同之处）</p> <p>（8）<code>hdfs dfs -deleteSnapshot &lt;path&gt; &lt;snapshotName&gt;</code>  （功能描述：删除快照）</p> <h2 id="回收站"><a href="#回收站" aria-hidden="true" class="header-anchor">#</a> 回收站</h2> <p>1）默认回收站</p> <p>默认值fs.trash.interval=0，0表示禁用回收站，可以设置删除文件的存活时间。</p> <p>默认值fs.trash.checkpoint.interval=0，检查回收站的间隔时间。</p> <p>要求fs.trash.checkpoint.interval&lt;=fs.trash.interval。</p> <p>2）启用回收站</p> <p>修改core-site.xml，配置垃圾回收时间为1分钟。</p> <div class="language-xml extra-class"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>fs.trash.interval<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>1<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre></div><p>回收站在集群中的路径：/user/root/.Trash/….</p> <p>3）修改访问垃圾回收站用户名称</p> <p>​	进入垃圾回收站用户名称，默认是dr.who，修改为root用户</p> <p>​	[core-site.xml]</p> <div class="language-xml extra-class"><pre class="language-xml"><code><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>property</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">&gt;</span></span>hadoop.http.staticuser.user<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">&gt;</span></span>
  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">&gt;</span></span>root<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">&gt;</span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>property</span><span class="token punctuation">&gt;</span></span>
</code></pre></div><p>4）编程方式</p> <div class="language- extra-class"><pre class="language-text"><code>Trash trash = New Trash(conf);

trash.moveToTrash(path);
</code></pre></div><p>5）恢复回收站数据</p> <div class="language- extra-class"><pre class="language-text"><code>hadoop fs -mv /user/root/.Trash/Current/user/root/input    /user/root/input
</code></pre></div><p>6）清空回收站</p> <div class="language- extra-class"><pre class="language-text"><code>hdfs dfs -expunge
</code></pre></div><h2 id="hdfs-ha高可用"><a href="#hdfs-ha高可用" aria-hidden="true" class="header-anchor">#</a> HDFS HA高可用</h2> <h3 id="hadoop1-x"><a href="#hadoop1-x" aria-hidden="true" class="header-anchor">#</a> Hadoop1.x</h3> <p>Secondary NameNode：它不是HA，它只是阶段性的合并edits和fsimage，以缩短集群启动的时间。当NN失效的时候，Secondary NN并无法立刻提供服务，Secondary NN甚至无法保证数据完整性：如果NN数据丢失的话，在上一次合并后的文件系统的改动会丢失</p> <p>Backup NameNode (HADOOP-4539)：它在内存中复制了NN的当前状态，算是Warm Standby，可也就仅限于此，并没有failover等。它同样是阶段性的做checkpoint，也无法保证数据完整性</p> <p>手动把name.dir指向NFS（Network File System），这是安全的Cold Standby，可以保证元数据不丢失，但集群的恢复则完全靠手动</p> <p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g5abg7rtjbj306r05h3z6.jpg" alt="pastedGraphic.png"></p> <h3 id="hadoop2-x"><a href="#hadoop2-x" aria-hidden="true" class="header-anchor">#</a> hadoop2.X</h3> <p>hadoop2.x之后，Clouera提出了QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案，它给出了一种较好的解决思路和方案,示意图如下：</p> <p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5abgecyf2j30av07uq5e.jpg" alt="image-20190705164200016"></p> <p>基本原理就是用2N+1台 JN 存储EditLog，每次写数据操作有大多数（&gt;=N+1）返回成功时即认为该次写成功，数据不会丢失了。当然这个算法所能容忍的是最多有N台机器挂掉，如果多于N台挂掉，这个算法就失效了。这个原理是基于Paxos算法</p> <p>在HA架构里面SecondaryNameNode这个冷备角色已经不存在了，为了保持standby NN时时的与主Active NN的元数据保持一致，他们之间交互通过一系列守护的轻量级进程JournalNode</p> <p>任何修改操作在 Active NN上执行时，JN进程同时也会记录修改log到至少半数以上的JN中，这时 Standby NN 监测到JN 里面的同步log发生变化了会读取 JN 里面的修改log，然后同步到自己的的目录镜像树里面，如下图：</p> <p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g5abgh7xmfj308404u3z0.jpg" alt="image-20190705164212333"></p> <p>当发生故障时，Active的 NN 挂掉后，Standby NN 会在它成为Active NN 前，读取所有的JN里面的修改日志，这样就能高可靠的保证与挂掉的NN的目录镜像树一致，然后无缝的接替它的职责，维护来自客户端请求，从而达到一个高可用的目的</p> <p>QJM方式来实现HA的主要优势：</p> <ol><li><p>不需要配置额外的高共享存储，降低了复杂度和维护成本</p></li> <li><p>消除spof</p></li> <li><p>系统鲁棒性(Robust:健壮)的程度是可配置</p></li> <li><p>JN不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JN的数量增多而影响性能（因为NN向JN发送日志是并行的）</p></li></ol> <h3 id="hadoop2-x-ha-详述"><a href="#hadoop2-x-ha-详述" aria-hidden="true" class="header-anchor">#</a> hadoop2.x ha 详述</h3> <p>datanode的fencing: 确保只有一个NN能命令DN。HDFS-1972中详细描述了DN如何实现fencing</p> <ol><li><p>每个NN改变状态的时候，向DN发送自己的状态和一个序列号</p></li> <li><p>DN在运行过程中维护此序列号，当failover时，新的NN在返回DN心跳时会返回自己的active状态和一个更大的序列号。DN接收到这个返回则认为该NN为新的active</p></li> <li><p>如果这时原来的active NN恢复，返回给DN的心跳信息包含active状态和原来的序列号，这时DN就会拒绝这个NN的命令</p></li></ol> <p>客户端fencing：确保只有一个NN能响应客户端请求，让访问standby nn的客户端直接失败。在RPC层封装了一层，通过FailoverProxyProvider以重试的方式连接NN。通过若干次连接一个NN失败后尝试连接新的NN，对客户端的影响是重试的时候增加一定的延迟。客户端可以设置重试次数和时间</p> <p>Hadoop提供了ZKFailoverController角色，部署在每个NameNode的节点上，作为一个deamon进程, 简称zkfc，示例图如下：</p> <p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g5abgk1cqrj308q05n3z3.jpg" alt="pastedGraphic.png"></p> <p>FailoverController主要包括三个组件:</p> <p>HealthMonitor: 监控NameNode是否处于unavailable或unhealthy状态。当前通过RPC调用NN相应的方法完成</p> <p>ActiveStandbyElector: 管理和监控自己在ZK中的状态</p> <p>ZKFailoverController 它订阅HealthMonitor 和ActiveStandbyElector 的事件，并管理NameNode的状态</p> <p>ZKFailoverController主要职责：</p> <ol><li><p>健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态</p></li> <li><p>会话管理：如果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主NN，同时标记状态为Active</p></li> <li><p>当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN</p></li> <li><p>master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态</p></li></ol> <h3 id="hdfs-ha工作要点"><a href="#hdfs-ha工作要点" aria-hidden="true" class="header-anchor">#</a> HDFS-HA工作要点</h3> <p>1）元数据管理方式需要改变：</p> <ul><li>内存中各自保存一份元数据；</li> <li>Edits日志只有Active状态的namenode节点可以做写操作；</li> <li>两个namenode都可以读取edits；</li> <li>共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；</li></ul> <p>2）需要一个状态管理功能模块</p> <p>实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。</p> <p>3）必须保证两个NameNode之间能够ssh无密码登录。</p> <p>4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务</p> <h3 id="自动故障转移工作机制"><a href="#自动故障转移工作机制" aria-hidden="true" class="header-anchor">#</a> 自动故障转移工作机制</h3> <p>动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：</p> <p><strong>1</strong>）故障检测：集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。</p> <p><strong>2</strong>）现役<strong>NameNode</strong>选择：ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。</p> <p>ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：</p> <p><strong>1</strong>）健康监测：ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。</p> <p><strong>2</strong>）<strong>ZooKeeper</strong>会话管理：当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。</p> <p><strong>3</strong>）基于<strong>ZooKeeper</strong>的选择：如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。</p> <h2 id="hdfs-federation架构设计"><a href="#hdfs-federation架构设计" aria-hidden="true" class="header-anchor">#</a> HDFS Federation架构设计</h2> <p>单Active NN的架构使得HDFS在集群扩展性和性能上都有潜在的问题，当集群大到一定程度后，NN进程使用的内存可能会达到上百G，NN成为了性能的瓶颈</p> <p>常用的估算公式为1G对应1百万个块，按缺省块大小计算的话，大概是64T (这个估算比例是有比较大的富裕的，其实，即使是每个文件只有一个块，所有元数据信息也不会有1KB/block)</p> <p>为了解决这个问题，Hadoop 2.x提供了HDFS Federation, 示意图如下：</p> <p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g5abgn88rgj309b06lmy3.jpg" alt="pastedGraphic.png"></p> <p>多个NN共用一个集群里的存储资源，每个NN都可以单独对外提供服务</p> <p>每个NN都会定义一个存储池，有单独的id，每个DN都为所有存储池提供存储</p> <p>DN会按照存储池id向其对应的NN汇报块信息，同时，DN会向所有NN汇报本地存储可用资源情况</p> <p>如果需要在客户端方便的访问若干个NN上的资源，可以使用客户端挂载表，把不同的目录映射到不同的NN，但NN上必须存在相应的目录</p> <p>HDFS中央缓存管理</p> <p>HDFS提供了一个高效的缓存加速机制——Centralized Cache Management，可以将一些经常被读取的文件（例如Hive中的fact表）pin到内存中。这些DataNode的缓存也是由NameNode所管理的（NameNode所管理的cache依然是以block形式，而DataNode也会定期向NameNode汇报缓存状态），而客户端可以高效得读取被缓存的数据块；为了能锁定内存，该实现依赖于JNI使用libhadoop.so，所以POSIX资源限制也要进行相应的设置（ulimit -l），并确保下面的参数被设置。</p> <p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g5abgp3uxzj30au08h3z9.jpg" alt="pastedGraphic.png"></p> <p><code>dfs.datanode.max.locked.memory</code>该参数用于确定每个DataNode给缓存使用的最大内存量。设置这个参数和<code>ulimit -l</code>时，需要注意内存空间还需要一些内存用于做其他事情，比如，DataNode和应用程序JVM堆内存、以及操作系统的页缓存，以及计算框架的任务。所以不要使用太高的内存百分比。</p> <h3 id="原理结构"><a href="#原理结构" aria-hidden="true" class="header-anchor">#</a> 原理结构</h3> <p>HDFS Federation意味着在集群中将会有多个namenode/namespace，这样的方式有什么好处呢?</p> <p>多namespace的方式可以直接减轻单一NameNode的压力.</p> <p>一个典型的例子就是上面提到的NameNode内存过高问题,我们完全可以将上面部分大的文件目录移到另外一个NameNode上做管理。更重要的一点在于，这些NameNode是共享集群中所有的DataNode的，它们还是在同一个集群内的。HDFS Federation原理结构图如下:</p> <p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g5abgrkvrpj30bj080tav.jpg" alt="image-20190705164540361"></p> <p>我们可以拿这种图与上一小节的图做对比，我们可以得出这样一个结论:</p> <p>HDFS Federation是解决NameNode单点问题的水平横向扩展方案。</p> <p>这时候在DataNode上就不仅仅存储一个Block Pool下的数据了，而是多个(大家可以在DataNode的datadir所在目录里面查看BP-xx.xx.xx.xx打头的目录)。</p> <p>在HDFS Federation的情况下，只有元数据的管理与存放被分隔开了，但真实数据的存储还是共用的，这与viewFs还是不一样的。之前看别的文章在讲述HDFS Federation的时候直接拿viewFs来讲，个人觉得二者还是有些许的不同的，用一句话概况应该这么说：</p> <p>HDFS的viewFs是namespace完全独立(私人化)的Federation方案，可以这么说，viewFs是Federation的一个简单实现方案。</p> <p>因为它们不仅仅是namespace独立，而且真实数据的存放也是独立的，也就是多个完全独立的集群。在这点上我们还是有必要做一下区分，否则让人以为HDFS Federation就是viewFs.</p> <h3 id="方案优势"><a href="#方案优势" aria-hidden="true" class="header-anchor">#</a> 方案优势</h3> <p>第一点，命名空间的扩展。因为随着集群使用时间的加长，HDFS上存放的数据也将会越来越多。这个时候如果还是将所有的数据都往一个NameNode上存放，这个文件系统会显得非常的庞大，这时候我们可以进行横向扩展，把一些大的目录分离出去，使得每个NameNode下的数据看起来更加的精简。</p> <p>第二点，性能的提升。这个也很好理解，当NameNode所持有的数据量达到了一个非常大规模的量级的时候(比如超过1亿个文件)，这个时候NameNode的处理效率可能就会有影响,它可能比较容易的会陷入一个繁忙的状态。而整个集群将会受限于一个单点NameNode的处理效率，从而影响集群整体的吞吐量。这个时候多NameNode机制显然可以减轻很多这部分的压力。</p> <p>第三点，资源的隔离。这一点考虑的就比较深了，通过多个命名空间，我们可以将关键数据文件目录移到不同的NameNode上，以此不让这些关键数据的读写操作受到其他普通文件读写操作的影响。也就是说这些NameNode将会只处理特定的关键的任务所发来的请求，而屏蔽了其他普通任务的文件读写请求，以此做到了资源的隔离。千万不要小看这一点,当你发现NameNode正在处理某个不良任务的大规模的请求操作导致响应速度极慢时，你一定会非常的懊恼。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/wiki/hadoop/hadoop-2-Hadoop概述.html" class="prev">
          Hadoop
        </a></span> <span class="next"><a href="/wiki/hadoop/hadoop-4-MapReduce.html">
          MapReduce
        </a>
        →
      </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/wiki/assets/js/app.dd178f5a.js" defer></script><script src="/wiki/assets/js/2.9f273a62.js" defer></script><script src="/wiki/assets/js/22.f6801be0.js" defer></script>
  </body>
</html>
